{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clcinpdx/datascience/blob/master/RAG_Maven_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7y1LEMWjDK84",
        "outputId": "5afa2add-9d02-4130-b222-97e8d0b2a385"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:2 https://cli.github.com/packages stable InRelease\n",
            "Hit:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists...\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "pandoc is already the newest version (2.9.2.1-3ubuntu2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement re (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for re\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.26.0)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.185.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.11.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (2.32.5)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.75.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.10.5)\n",
            "\n",
            "Installation of libraries complete.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "#!pip install -r \"/content/drive/MyDrive/Colab Notebooks/requirements.txt\"\n",
        "!apt-get -q update\n",
        "!apt-get -q install -y pandoc\n",
        "!pip install -q requests\n",
        "!pip install -q chromadb\n",
        "!pip install -q braintrust\n",
        "!pip install -q autoevals\n",
        "!pip install -q openai\n",
        "!pip install -q dotenv\n",
        "!pip install -q langchain_community\n",
        "!pip install -q langchain_text_splitters\n",
        "!pip install -q langchain_openai\n",
        "!pip install -q unstructured\n",
        "!pip install -q pymupdf\n",
        "!pip install -q nltk\n",
        "!pip install -q re\n",
        "!pip install -q pymupdf\n",
        "!pip install -q rank_bm25\n",
        "!pip install -q google-generativeai\n",
        "\n",
        "\n",
        "print(\"\\nInstallation of libraries complete.\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88b2afb5",
        "outputId": "8f8add16-7fff-49a8-dd19-18ace6137e79"
      },
      "source": [
        "!pip uninstall -y langchain langchain-core langchain-community langchain-openai\n",
        "!pip install -q langchain langchain-core langchain-community langchain-openai"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: langchain 1.0.2\n",
            "Uninstalling langchain-1.0.2:\n",
            "  Successfully uninstalled langchain-1.0.2\n",
            "Found existing installation: langchain-core 1.0.1\n",
            "Uninstalling langchain-core-1.0.1:\n",
            "  Successfully uninstalled langchain-core-1.0.1\n",
            "Found existing installation: langchain-community 0.4\n",
            "Uninstalling langchain-community-0.4:\n",
            "  Successfully uninstalled langchain-community-0.4\n",
            "Found existing installation: langchain-openai 1.0.1\n",
            "Uninstalling langchain-openai-1.0.1:\n",
            "  Successfully uninstalled langchain-openai-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LWay-G1bVE7",
        "outputId": "3a642e14-bcee-4620-a2a2-009b875bf86a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "#from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
        "#from langchain_community.document_loaders import PyPDFLoader\n",
        "#from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk # Import the nltk library\n",
        "nltk.download('punkt') # Download the punkt tokenizer data\n",
        "nltk.download('punkt_tab') # Download the punkt_tab tokenizer data\n",
        "\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "import requests\n",
        "import json\n",
        "from google.colab import userdata\n",
        "import os # Import the os module\n",
        "import chromadb\n",
        "import re\n",
        "import sys\n",
        "from rank_bm25 import BM25Okapi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "fI8ROMADKBN-",
        "outputId": "6f0e0e02-031a-4da9-d6cd-48b7d313a13d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>pre { white-space: pre-wrap; }</style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "# Run this once at the top of your notebook\n",
        "display(HTML(\"<style>pre { white-space: pre-wrap; }</style>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jQVmV1yOcK1m"
      },
      "outputs": [],
      "source": [
        "# Load environment variables - project contains .env file with API keys\n",
        "dotenv_path = '/content/drive/MyDrive/Colab Notebooks/.env'\n",
        "load_dotenv(dotenv_path)\n",
        "\n",
        "OPENROUTER_API_KEY = userdata.get('OPENROUTER_API_KEY')\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "CHROMADB_API_KEY = userdata.get('CHROMADB_API_KEY')\n",
        "BRAINTRUST_API_KEY = userdata.get('BRAINTRUST_API_KEY')\n",
        "os.environ['BRAINTRUST_API_KEY'] = BRAINTRUST_API_KEY\n",
        "BRAINTRUST_API_URL='https://api.braintrust.dev'\n",
        "\n",
        "chroma_client = chromadb.CloudClient(api_key=CHROMADB_API_KEY,\n",
        "  tenant='35ea46f3-d512-4358-b1c4-037c01bc0a79', database='mavin-test')\n",
        "\n",
        "#set path variables\n",
        "DATA_PATH = \"/content/drive/MyDrive/Colab Notebooks/stockdata/NVIDIA\"\n",
        "\n",
        "model_for_qna = \"anthropic/claude-sonnet-4.5\"\n",
        "model_for_query = \"anthropic/claude-sonnet-4.5\"\n",
        "model_for_eval = \"anthropic/claude-sonnet-4.5\"\n",
        "#model_for_eval = \"openai/gpt-5\"\n",
        "collection_name = \"nvidia\"\n",
        "bm25_collection_name = \"bm25-nvidia\"\n",
        "returned_results_semantic = 10\n",
        "returned_results_bm25 = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cPh6Jn4_AV75"
      },
      "outputs": [],
      "source": [
        "  # Build the system prompt\n",
        "\n",
        "base_system_prompt = \"\"\"\n",
        "  You are a helpful assistant answering questions about the company NVIDIA.\n",
        "  But you only answer based on knowledge I'm providing you. You don't use your internal knowledge and you don't make things up.\n",
        "\n",
        "    IMPORTANT: Provide concise, factual answers. Use specific numbers, dates, and facts directly from the data. Avoid explanations, elaborations, or conversational filler. Answer in as few words as possible.\n",
        "\n",
        "    Example:\n",
        "    Question: What was NVIDIA's revenue in fiscal 2025?\n",
        "    Good answer: $130.5 billion\n",
        "    Bad answer: NVIDIA had a great year in fiscal 2025, achieving remarkable revenue growth. The company's total revenue reached approximately $130.5 billion, which represents...\n",
        "\n",
        "--------------------\n",
        "The data:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "id": "gQwhHbMjgLjb",
        "outputId": "598a694b-b39a-40e5-cc27-60e98a2d9dc5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'Schema' from 'google.generativeai' (/usr/local/lib/python3.12/dist-packages/google/generativeai/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4126533021.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerativeai\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgenai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerativeai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSchema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGenerateContentConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mType\u001b[0m \u001b[0;31m# <-- Import Schema, GenerateContentConfig, and Type directly from google.generativeai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocuments\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocument\u001b[0m \u001b[0;31m# Import Document here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'Schema' from 'google.generativeai' (/usr/local/lib/python3.12/dist-packages/google/generativeai/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "from google.generativeai import Schema, GenerateContentConfig, Type # <-- Import Schema, GenerateContentConfig, and Type directly from google.generativeai\n",
        "from langchain_core.documents import Document # Import Document here\n",
        "\n",
        "def main():\n",
        "\n",
        "  def print_skynet_logo():\n",
        "    print(\"\"\"\n",
        "    ███████╗██╗  ██╗██╗   ██╗███╗   ██╗███████╗████████╗\n",
        "    ██╔════╝██║ ██╔╝╚██╗ ██╔╝████╗  ██║██╔════╝╚══██╔══╝\n",
        "    ███████╗█████╔╝  ╚████╔╝ ██╔██╗ ██║█████╗     ██║\n",
        "    ╚════██║██╔═██╗   ╚██╔╝  ██║╚██╗██║██╔══╝     ██║\n",
        "    ███████║██║  ██╗   ██║   ██║ ╚████║███████╗   ██║\n",
        "    ╚══════╝╚═╝  ╚═╝   ╚═╝  ╚═╝  ╚═══╝╚═══════════╝\n",
        "\n",
        "    ═══════════════════════════════════════════════════\n",
        "              CYBERDYNE SYSTEMS - SERIES 800\n",
        "    ═══════════════════════════════════════════════════\n",
        "    \"\"\")\n",
        "    return\n",
        "\n",
        "  def goodbye():\n",
        "      print(\"\\nThank you for using Skynet!\")\n",
        "      print(\"\\nAll results produced via Skynet Systems @2025, a quality Cyberdyne subsidiary with humanity's best interest always at heart.\")\n",
        "\n",
        "  # Modified function to use Google Gemini 2.0 Flash for PDF processing\n",
        "  def load_and_process_documents(data_path):\n",
        "    \"\"\"Loads and processes PDF documents using Google Gemini 2.0 Flash with guaranteed JSON output.\"\"\"\n",
        "    from google.generativeai import GenerativeModel\n",
        "    # Ensure Document is accessible if not imported globally\n",
        "\n",
        "    # Define the schema for the expected JSON output: an array of objects, each with a 'text' key.\n",
        "    # Removed bounding_box from schema as per user request to remove bounding box code\n",
        "    chunk_schema = Schema(\n",
        "        type=Type.ARRAY, # Use Type directly after explicit import\n",
        "        description=\"A list of document chunks, where each chunk is an object with a 'text' field.\",\n",
        "        items=Schema(\n",
        "            type=Type.OBJECT, # Use Type directly after explicit import\n",
        "            properties={\n",
        "                \"text\": Schema( # Use Schema for the property definition\n",
        "                    type=Type.STRING, # Use Type directly after explicit import\n",
        "                    description=\"The raw extracted text content of a single document chunk.\" # Correctly placed description\n",
        "                )\n",
        "            },\n",
        "            required=[\"text\"]\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Initialize the Gemini API\n",
        "    try:\n",
        "        # ... (API initialization remains the same)\n",
        "        GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "        genai.configure(api_key=GOOGLE_API_KEY)\n",
        "        gemini_model = GenerativeModel('gemini-2.0-flash')\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing Google Gemini API: {e}\")\n",
        "        print(\"Please ensure you have added your GOOGLE_API_KEY to Colab secrets.\")\n",
        "        return []\n",
        "\n",
        "    all_chunks = []\n",
        "    for filename in os.listdir(data_path):\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            file_path = os.path.join(data_path, filename)\n",
        "            print(f\"Processing {filename}...\")\n",
        "            try:\n",
        "                # Use Gemini to process the PDF\n",
        "                with open(file_path, \"rb\") as f:\n",
        "                    response = gemini_model.generate_content(\n",
        "                        contents=[\n",
        "                            {\n",
        "                                \"role\": \"user\",\n",
        "                                \"parts\": [\n",
        "                                    {\"mime_type\": \"application/pdf\", \"data\": f.read()},\n",
        "                                    # Modified prompt to be more direct\n",
        "                                    {\"text\": \"Extract and return the text for each chunk of this PDF document, strictly following the provided JSON schema.\"},\n",
        "                                ],\n",
        "                            }\n",
        "                        ],\n",
        "                        # Use the new config parameter with response_schema\n",
        "                        config=GenerateContentConfig(\n",
        "                            response_mime_type=\"application/json\",\n",
        "                            response_schema=chunk_schema\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "                # Attempt to load the JSON response\n",
        "                pdf_data = None\n",
        "                try:\n",
        "                    pdf_data = json.loads(response.text)\n",
        "                except json.JSONDecodeError as e:\n",
        "                    print(f\"Error decoding JSON from Gemini response for {filename}: {e}\")\n",
        "                    print(f\"Raw response text: {response.text}\") # Print raw response for debugging\n",
        "                    continue # Skip to the next file if JSON decoding fails\n",
        "\n",
        "                # Check if the parsed data is a list and process it\n",
        "                if isinstance(pdf_data, list):\n",
        "                    for item in pdf_data:\n",
        "                        if isinstance(item, dict) and \"text\" in item: # Check if item is a dictionary and has 'text' key\n",
        "                            all_chunks.append(Document(\n",
        "                                page_content=item[\"text\"],\n",
        "                                metadata={\n",
        "                                    \"source\": filename,\n",
        "                                }\n",
        "                            ))\n",
        "                        else:\n",
        "                            print(f\"Skipping item in {filename} due to unexpected structure: {item}\")\n",
        "                else: # Handle cases where pdf_data is not a list\n",
        "                    print(f\"Skipping processing for {filename} due to unexpected JSON structure: Expected a list, but received {type(pdf_data)}\")\n",
        "                    print(f\"Received data: {pdf_data}\") # Print the received data for debugging\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename} with Gemini: {e}\")\n",
        "\n",
        "    print(f\"Processed {len(all_chunks)} chunks with metadata.\")\n",
        "    return all_chunks\n",
        "\n",
        "\n",
        "  # Removed sentence_aware_chunking as Gemini handles chunking\n",
        "  # def sentence_aware_chunking(documents, max_len):\n",
        "  #   all_chunks = []\n",
        "  #   for doc in documents:\n",
        "  #       sentences = sent_tokenize(doc.page_content)\n",
        "  #       current_chunk = []\n",
        "  #       current_len = 0\n",
        "  #       for sentence in sentences:\n",
        "  #           # Check if adding the current sentence exceeds the max length\n",
        "  #           if current_len + len(sentence) + (1 if current_chunk else 0) > max_len:\n",
        "  #               # If it does, add the current accumulated chunk to all_chunks\n",
        "  #               if current_chunk:\n",
        "  #                   all_chunks.append(Document(page_content=' '.join(current_chunk), metadata=doc.metadata))\n",
        "  #               # Start a new chunk with the current sentence\n",
        "  #               current_chunk = [sentence]\n",
        "  #               current_len = len(sentence)\n",
        "  #           else:\n",
        "  #               # Otherwise, add the current sentence to the current chunk\n",
        "  #               current_chunk.append(sentence)\n",
        "  #               current_len += len(sentence) + (1 if current_chunk else 0)  # Add space if not the first sentence\n",
        "\n",
        "  #       # Add the last chunk if it's not empty\n",
        "  #       if current_chunk:\n",
        "  #             all_chunks.append(Document(page_content=' '.join(current_chunk), metadata=doc.metadata))\n",
        "  #   return all_chunks\n",
        "\n",
        "  def save_to_chroma(chunks):\n",
        "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
        "    batch_size = 300\n",
        "    for i in range(0, len(chunks), batch_size):\n",
        "      batch = chunks[i:i + batch_size]\n",
        "      documents = []\n",
        "      metadata = []\n",
        "      ids = []\n",
        "      for j, chunk in enumerate(batch):\n",
        "          documents.append(chunk.page_content)\n",
        "          ids.append(\"ID\" + str(i + j)) # Ensure unique IDs across all batches\n",
        "          metadata.append(chunk.metadata)\n",
        "\n",
        "      print(f\"Attempting to add batch {int(i/batch_size) + 1} of {int(len(chunks)/batch_size) + 1} with {len(batch)} chunks.\")\n",
        "      try:\n",
        "          collection.upsert(documents=documents, metadatas=metadata, ids=ids)\n",
        "          print(f\"Successfully added batch {int(i/batch_size) + 1}.\")\n",
        "      except Exception as e:\n",
        "          print(f\"Error adding batch {int(i/batch_size) + 1}: {e}\")\n",
        "          # Depending on the error, you might want to break or handle it differently\n",
        "\n",
        "    return collection\n",
        "\n",
        "  # Create Keyword Tokenized Corpus\n",
        "  def create_keyword_corpus(chunks): # Modified to accept chunks as an argument\n",
        "    # Extract text content from chunks\n",
        "    corpus = [chunk.page_content for chunk in chunks]\n",
        "    # Tokenize each document in the corpus\n",
        "    tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
        "    # Store the tokenized corpus in a variable\n",
        "    # This is already done by assigning to tokenized_corpus\n",
        "    print(f\"Prepared a tokenized corpus of {len(tokenized_corpus)} documents for BM25.\")\n",
        "    # Create a BM25 model instance\n",
        "    bm25 = BM25Okapi(tokenized_corpus)\n",
        "    return bm25, tokenized_corpus\n",
        "\n",
        "\n",
        "  # Save BM25 data\n",
        "  def save_bm25_data(bm25_model, tokenized_corpus, collection_name):\n",
        "      \"\"\"Saves the BM25 model and tokenized corpus to disk.\"\"\"\n",
        "      bm25_dir = f\"./bm25_data/{collection_name}\"\n",
        "      os.makedirs(bm25_dir, exist_ok=True)\n",
        "      # Save the tokenized corpus\n",
        "      with open(f\"{bm25_dir}/tokenized_corpus.json\", \"w\") as f:\n",
        "        json.dump(tokenized_corpus, f)\n",
        "      # BM25Okapi model itself doesn't have a simple save/load method.\n",
        "      # Rebuilding from the tokenized corpus is the standard approach.\n",
        "      print(f\"Tokenized corpus saved to {bm25_dir}/tokenized_corpus.json\")\n",
        "\n",
        "\n",
        "  # Load BM25 data\n",
        "  def load_bm25_data(collection_name):\n",
        "      \"\"\"Loads the tokenized corpus and rebuilds the BM25 model from disk.\"\"\"\n",
        "      bm25_dir = f\"./bm25_data/{collection_name}\"\n",
        "      corpus_path = f\"{bm25_dir}/tokenized_corpus.json\"\n",
        "      if not os.path.exists(corpus_path):\n",
        "        print(f\"BM25 data not found at {corpus_path}\")\n",
        "        return None, None\n",
        "      with open(corpus_path, \"r\") as f:\n",
        "        tokenized_corpus = json.load(f)\n",
        "      bm25_model = BM25Okapi(tokenized_corpus)\n",
        "      print(f\"BM25 model loaded from {corpus_path}\")\n",
        "      return bm25_model, tokenized_corpus\n",
        "\n",
        "  def rerank_results(chroma_results, bm25_results, query):\n",
        "    \"\"\"Combines and reranks results from ChromaDB and BM25.\"\"\"\n",
        "    combined_docs = []\n",
        "    seen_content = set()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMBINED SEARCH RESULTS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Add ChromaDB results\n",
        "    chroma_count = 0\n",
        "    if chroma_results and chroma_results.get('documents'):\n",
        "        print(\"\\n📊 SEMANTIC SEARCH (ChromaDB) RESULTS:\")\n",
        "        print(\"-\"*80)\n",
        "        for doc_list in chroma_results['documents']:\n",
        "            for doc_content in doc_list:\n",
        "                if doc_content not in seen_content:\n",
        "                    chroma_count += 1\n",
        "                    combined_docs.append(doc_content)\n",
        "                    seen_content.add(doc_content)\n",
        "                    print(f\"\\n[Semantic #{chroma_count}]\")\n",
        "                    print(f\"{doc_content}\")\n",
        "                    print(\"-\"*80)\n",
        "\n",
        "    # Add BM25 results (AKA Keyword search)\n",
        "    bm25_count = 0\n",
        "    if bm25_results:\n",
        "        print(\"\\n🔍 KEYWORD SEARCH (BM25) RESULTS:\")\n",
        "        print(\"-\"*80)\n",
        "        for doc_content in bm25_results:\n",
        "            # BM25 results should already be strings from your corpus\n",
        "            # If they're tokenized, join them\n",
        "            if isinstance(doc_content, list):\n",
        "                doc_content = \" \".join(doc_content)\n",
        "\n",
        "            if doc_content not in seen_content:\n",
        "                bm25_count += 1\n",
        "                combined_docs.append(doc_content)\n",
        "                seen_content.add(doc_content)\n",
        "                print(f\"\\n[Keyword #{bm25_count}]\")\n",
        "                print(f\"{doc_content}\")\n",
        "                print(\"-\"*80)\n",
        "\n",
        "    # Summary\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"SUMMARY: {chroma_count} semantic + {bm25_count} keyword = {len(combined_docs)} total unique documents\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    return combined_docs\n",
        "\n",
        "\n",
        "  # Define the function to perform BM25 search\n",
        "  def perform_bm25_search(bm25, tokenized_corpus, query_string,returned_results_bm25): # Added bm25 model as argument\n",
        "    \"\"\"Performs a BM25 search on the tokenized corpus\n",
        "    Args:\n",
        "    bm25: The BM25 model instance.\n",
        "    tokenized_corpus: The tokenized corpus used to build the BM25 model.\n",
        "    query_string: The query string to search for.\n",
        "    n: The number of top results to return.\n",
        "\n",
        "    Returns:\n",
        "    A list of top n documents from the corpus based on the BM25 score.\n",
        "    \"\"\"\n",
        "    tokenized_query = query_string.split(\" \")\n",
        "    top_docs = bm25.get_top_n(tokenized_query, tokenized_corpus, returned_results_bm25) # Added tokenized_corpus\n",
        "    return top_docs\n",
        "\n",
        "\n",
        "\n",
        "  #################################\n",
        "  # Evaluation -> Braintrust (bt) #\n",
        "  #################################\n",
        "\n",
        "  def get_braintrust_results_task(braintrust_input):\n",
        "\n",
        "      collection = chroma_client.get_collection(name=collection_name)\n",
        "      bm25_model, tokenized_corpus = load_bm25_data(bm25_collection_name)\n",
        "      if bm25_model is None or tokenized_corpus is None:\n",
        "        print(\"BM25 data not found. Please build the Keyword DB first ('k' option).\")\n",
        "\n",
        "      # Perform Semantic (vector) Search\n",
        "      semantic_results = collection.query(\n",
        "      query_texts=[braintrust_input],\n",
        "      n_results=returned_results_semantic\n",
        "      )\n",
        "\n",
        "      # Perform BM25 search\n",
        "      bm25_results = perform_bm25_search(bm25_model, tokenized_corpus, braintrust_input, returned_results_bm25) # Pass bm25_model and tokenized_corpus\n",
        "\n",
        "      # Rerank the combined results\n",
        "      combined_context_docs = rerank_results(semantic_results, bm25_results, braintrust_input)\n",
        "\n",
        "      # Build the system prompt using the combined context\n",
        "      context_string = \"\\n\".join(combined_context_docs)\n",
        "      system_prompt = f\"{base_system_prompt}{context_string}\"\n",
        "\n",
        "      print(f\"braintrust_input: {braintrust_input}\")\n",
        "\n",
        "\n",
        "      response = requests.post(\n",
        "        url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
        "        headers={\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"HTTP-Referer\": \"https://colab.research.google.com/\",\n",
        "        \"X-Title\": \"Colab Notebook\",\n",
        "        },\n",
        "        data=json.dumps({\n",
        "        \"model\": model_for_query,\n",
        "        \"messages\": [\n",
        "            {\"role\":\"system\",\"content\":system_prompt},\n",
        "            {\"role\":\"user\",\"content\":braintrust_input}\n",
        "          ]\n",
        "        })\n",
        "        )\n",
        "\n",
        "      output = response.json()['choices'][0]['message']['content']\n",
        "      return output\n",
        "\n",
        "\n",
        "  #############################\n",
        "  # Evalulation -> Braintrust #\n",
        "  #############################\n",
        "\n",
        "  def eval_model():\n",
        "      from braintrust import Eval, init_dataset\n",
        "      from autoevals import (Factuality, Faithfulness, ContextRelevancy, AnswerRelevancy, Levenshtein)\n",
        "\n",
        "      eval_run = Eval(\n",
        "        \"mavin\",\n",
        "        data=init_dataset(project=\"mavin\", name=\"Synthetic-Eval-Q_n_Answers-Sonnet45\"),\n",
        "        task=get_braintrust_results_task,\n",
        "        scores=[Factuality],\n",
        "        #scores=[Factuality,Faithfulness,ContextRelevancy,AnswerRelevancy],\n",
        "        experiment_name=f\"Eval-{collection_name}-LLM_used-{model_for_eval}__keyword_result_count-{returned_results_bm25}__vector_result_count-{returned_results_semantic}\",\n",
        "        max_concurrency=5\n",
        "        )\n",
        "      return\n",
        "\n",
        "\n",
        "  #############################\n",
        "  # Adhoc query of the system #\n",
        "  #############################\n",
        "\n",
        "  def adhoc_ask(collection, bm25_model, tokenized_corpus): # Added bm25_model and tokenized_corpus as arguments\n",
        "\n",
        "    user_query = input(f\"What do you want to know about {collection_name}?\\n\\n\")\n",
        "\n",
        "    # Perform Semantic (vector) Search\n",
        "    results = collection.query(\n",
        "    query_texts=[user_query],\n",
        "    n_results=returned_results_semantic,\n",
        "    include=['metadatas'] # Include metadata\n",
        "    )\n",
        "\n",
        "    # Perform BM25 search\n",
        "    bm25_results = perform_bm25_search(bm25_model, tokenized_corpus, user_query, returned_results_bm25) # Pass bm25_model and tokenized_corpus\n",
        "    #print(f\"BM25 Results:\\n\")\n",
        "    #for doc_tokens in bm25_results:\n",
        "    #  print(\" \".join(doc_tokens))\n",
        "    #print(\"\\n\")\n",
        "\n",
        "    # Rerank the combined results\n",
        "    combined_context_docs = rerank_results(results, bm25_results, user_query) # Pass user_query as query\n",
        "\n",
        "    # Build the system prompt using the combined context\n",
        "    context_string = \"\\n\".join(combined_context_docs)\n",
        "    system_prompt = f\"{base_system_prompt}{context_string}\"\n",
        "\n",
        "    print(system_prompt)\n",
        "\n",
        "    response = requests.post(\n",
        "      url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
        "      headers={\n",
        "      \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "      \"HTTP-Referer\": \"https://colab.research.google.com/\",\n",
        "      \"X-Title\": \"Colab Notebook\",\n",
        "      },\n",
        "      data=json.dumps({\n",
        "      \"model\": model_for_query,\n",
        "      \"messages\": [\n",
        "          {\"role\":\"system\",\"content\":system_prompt},\n",
        "          {\"role\":\"user\",\"content\":user_query}\n",
        "        ]\n",
        "      })\n",
        "      )\n",
        "\n",
        "    answer = response.json()['choices'][0]['message']['content']\n",
        "    print(f\"{answer}\")\n",
        "\n",
        "    # Removed code to extract and display bounding box information\n",
        "\n",
        "\n",
        "    return answer\n",
        "\n",
        "  print_skynet_logo()\n",
        "\n",
        "  while True:\n",
        "    user_input = input(\n",
        "        \"\\nPlease select an option:\\n\"\n",
        "        \"  'b' - Build VectorDB RAG\\n\"\n",
        "        \"  'k' - Build Keyword (BM25) DB\\n\"\n",
        "        \"  'e' - Run evals\\n\"\n",
        "        \"  'q' - Query the RAG directly\\n\"\n",
        "        \"  'x' - Exit\\n\"\n",
        "        \"Your choice: \"\n",
        "    ).strip().lower()\n",
        "\n",
        "    if user_input == 'b':\n",
        "        chunks = load_and_process_documents(DATA_PATH) # Use the new function\n",
        "        collection = save_to_chroma(chunks)\n",
        "        print(f\"Successfully built VectorDB RAG: {collection}\")\n",
        "\n",
        "    elif user_input == 'k': # Added option for building keyword DB\n",
        "        chunks = load_and_process_documents(DATA_PATH) # Use the new function to get chunks for BM25\n",
        "        bm25_model, tokenized_corpus = create_keyword_corpus(chunks)\n",
        "        save_bm25_data(bm25_model, tokenized_corpus, bm25_collection_name)\n",
        "        print(f\"Successfully built and saved Keyword (BM25) DB for collection: {bm25_collection_name}\")\n",
        "\n",
        "    elif user_input == 'e':\n",
        "        eval_model()\n",
        "        print(\"\\nSuccessfully initiated eval -> see results in Braintrust.\")\n",
        "        goodbye()\n",
        "        break\n",
        "\n",
        "\n",
        "\n",
        "    elif user_input == 'q':\n",
        "        while True:\n",
        "            collection = chroma_client.get_collection(name=collection_name)\n",
        "            bm25_model, tokenized_corpus = load_bm25_data(bm25_collection_name) # Load BM25 data\n",
        "            if bm25_model is None or tokenized_corpus is None:\n",
        "              print(\"BM25 data not found. Please build the Keyword DB first ('k' option).\")\n",
        "              break # Exit query loop if BM25 data is not available\n",
        "\n",
        "            adhoc_ask(collection, bm25_model, tokenized_corpus) # Pass BM25 data to ask function\n",
        "            query_input = input(\"\\nPress 'Enter' to continue asking questions, or 'x' to exit: \").strip().lower()\n",
        "            if query_input == 'x':\n",
        "              goodbye()\n",
        "              break\n",
        "\n",
        "    elif user_input == 'x':\n",
        "      goodbye()\n",
        "      break\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid input. Please choose 'b', 'k', 'e', 'q', or 'x'.\") # Updated valid options\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "434048da",
        "outputId": "fb1c6213-cd7a-46e2-8b08-a50be93941ad"
      },
      "source": [],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.26.0)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.185.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.11.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (2.32.5)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.75.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.10.5)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1CT6dXoA9MLY8NmeaSUEidoaT8SvDxAms",
      "authorship_tag": "ABX9TyMMKCUfVUG2mcD+rAM249m9",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}